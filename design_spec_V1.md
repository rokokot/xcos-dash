
## ğŸŒ xCoS System Implementation Blueprint

**Goal:** Operationalize the *Modelâ€“Solveâ€“Explain* loop into an interactive, human-centered dashboard for constraint solving.

---

### **1. Input and Model Initialization**

**Objective:** Gather and structure domain data (e.g., exam scheduling, nurse rostering) into a valid CPMpy model.

**Rules:**
1.1. Load domain data (exams, rooms, preferences) â†’ normalize into CPMpy-compatible tables.
1.2. Initialize decision variables (`cp.intvar`, `cp.boolvar`) and domain bounds.
1.3. Construct constraint sets:

* `HARD`: domain and structural constraints
* `SOFT`: preference-based or weighted constraints
  1.4. Add optional objective functions:
* single-objective (maximize satisfaction, minimize penalty)
* multi-objective (trade-off scores between satisfaction, fairness, utilization)

**Artifacts produced:**
`model`, `variables`, `constraints`, `objective`

**Data type produced:** *Constraint sets, variable domains, objectives*

---

### **2. Solve Phase**

**Objective:** Run the solver, capture results and metadata.
(Backbone: CPMpy + FastAPI backend)

**Rules:**
2.1. Call `model.solve()` or `solver.solve()` with metadata logging enabled.
2.2. Record:

* `status()` â†’ SAT / UNSAT
* `objective_value()` â†’ if applicable
* `var.value()` â†’ variable assignments
* search statistics (time, branches, conflicts)
  2.3. If incremental solving is enabled â†’ push intermediate results via WebSocket.

**Artifacts produced:**
`solution`, `solver_state`, `log_trace`

**Data type produced:** *Assignments, constraint satisfaction state, solver progress*

---

### **3. Feasibility & Conflict Diagnosis**

**Objective:** Provide first-line explanations for SAT/UNSAT outcomes.

**Rules:**
3.1. If **UNSAT**, compute and store:

* `MUS()` â†’ minimal unsatisfiable subsets
* `quickxplain()` â†’ preferred or lexicographically ordered MUS
* `smus()` / `optimal_mus()` â†’ minimal cost MUS (if weighted)
  3.2. If **SAT**, compute:
* satisfaction report (which constraints violated, by how much)
* soft constraint violation scores
* constraint dependency graph
  3.3. Store conflict data for visualization modules.

**Artifacts produced:**
`MUS`, `MCS`, `MSS`, `conflict_graph`, `violation_map`

**Data type produced:** *Constraint-level status & relational data*

---

### **4. Repair and Restoration**

**Objective:** Suggest minimal changes to restore feasibility or improve quality.

**Rules:**
4.1. For each MUS, compute corresponding MCS:
â†’ `mcs()` or enumerator (`MARCO`)
4.2. Derive **repair options**:

* constraint relaxations (drop, reweight, toggle)
* slack model introduction (`cp.sum(slacks)`)
  4.3. Rank repairs by:
* minimal disruption (fewest constraints removed)
* lowest cost
* user preference weight (from feedback history)
  4.4. Push repair options to frontend for preview.

**Artifacts produced:**
`repair_options`, `relaxation_scenarios`, `impact_scores`

**Data type produced:** *Repair graphs, MCS sets, weighted rankings*

---

### **5. Exploration and Optimization**

**Objective:** Enable interactive exploration of the solution space.

**Rules:**
5.1. If optimization â†’ compute Pareto frontiers via repeated solve calls.

* `pareto_solutions = multi_objective(model, objectives)`
  5.2. For trade-off exploration:
* display 2D scatter or radar plots (solution quality dimensions)
* allow user to adjust weights dynamically â†’ recompute frontier
  5.3. Generate counterfactual scenarios:
* `what_if(Xâ†’Y)` â†’ minimally adjust model for new goals
* compute delta via MUS/MCS difference set

**Artifacts produced:**
`pareto_front`, `scenario_deltas`, `counterfactuals`

**Data type produced:** *Solution-space relations, sensitivity analyses*

---

### **6. Visualization Layer**

**Objective:** Translate solver artifacts into interactive, human-readable forms.

**Rules (dataâ†’viz mapping):**

| Data Type                | Visualization Primitive    | Interaction                             |
| ------------------------ | -------------------------- | --------------------------------------- |
| Constraints & violations | Table + heatmap            | Click to expand MUS                     |
| Conflict sets (MUS/MCS)  | UpSet or node-link diagram | Select constraint to highlight overlaps |
| Trade-offs / Pareto sets | Scatterplot + radar chart  | Adjust weights, preview new solutions   |
| Counterfactuals          | Diff view                  | Compare scenarios                       |
| Solution schedule        | Gantt/Matrix grid          | Dragâ€“drop to reassign                   |

**Artifacts produced:**
`explanation widgets`, `interactive overlays`, `scenario diff views`

**Data type produced:** *Visual-analytic state*

---

### **7. Interaction and Feedback Loop**

**Objective:** Maintain human-in-the-loop agency and continual learning.

**Rules:**
7.1. Each visualization widget communicates back to solver state via API/WebSocket.
7.2. Store user actions as **feedback signals**:

* constraint lock/unlock
* objective reweight
* accepted repairs
  7.3. Feed feedback into learning module â†’ update preference models.
  7.4. Log sessions for iterative design refinement and future explanation personalization.

**Artifacts produced:**
`feedback_log`, `updated_weights`, `learning_state`

**Data type produced:** *Human preference metadata*

---

### **8. Export and Provenance**

**Objective:** Support traceability, reproducibility, and communication.

**Rules:**
8.1. Export model + explanation bundle (JSON):
`{ model, constraints, MUS, MCS, solution, user_actions }`
8.2. Enable visual provenance tracking: which explanation led to which repair.
8.3. Allow report generation:

* explanation summary
* constraintâ€“decision mapping
* Pareto trade-off overview

**Artifacts produced:**
`export_bundle`, `provenance_graph`, `explanation_report`

---

## ğŸ“˜ Simplified User Flow Summary

1ï¸âƒ£ **Model Setup** â†’ build + visualize constraint map
2ï¸âƒ£ **Solve** â†’ get results and solver trace
3ï¸âƒ£ **Explain** â†’ highlight conflicts or quality metrics
4ï¸âƒ£ **Repair** â†’ propose minimal relaxations
5ï¸âƒ£ **Explore** â†’ compare alternatives, counterfactuals
6ï¸âƒ£ **Reflect** â†’ adjust model, export results

---

## ğŸ§© Implementation Anchors

* **Backend:** CPMpy + FastAPI

  * Implements all MUS/MCS/counterfactual logic
  * Provides REST + WebSocket endpoints
* **Frontend:** React + D3 + Zustand

  * Dashboard organized as:
    **Left:** Constraints panel
    **Center:** Schedule / Graph view
    **Right:** Explanation & Repair panel
* **Visualization modules:**

  * ConflictInspector (MUS/MCS)
  * ParetoExplorer
  * ScenarioComparator
  * RepairMenu

---

## ğŸ§  Strategic Notes (ties to theory)

* This operational pipeline **embeds the theoretical formalism implicitly** â€” MUS/MCS duality, stepwise explanations, and counterfactual reasoning drive the data flow but remain transparent to the end-user.
* The **visualization grammar** (marks, channels, composition rules) ensures consistency and scalability (see your `Design Studies` and `Visual encoding primitives` documents).
* Each explanation phase corresponds to a **distinct family** (Feasibility, Conflict, Repair, Counterfactual, Trade-off), enabling modular evaluation and user testing later.

---

Would you like me to produce a **visual version** of this â€” e.g., a refined flowchart diagram (SVG or markdown graph) that follows this logic but remains readable for inclusion in design documentation or a paper appendix?


### ğŸ§© **Overview of the Visualization Sketch**

This diagram shows an **interactive explanationâ€“repair workspace** focused on **MUS-based conflict visualization and repair selection**. Itâ€™s designed to visually bridge *diagnosis* (MUS identification) and *repair planning* (MCS enumeration and ranking).

It integrates three functional layers:

1. **Constraint-level visualization** (red/green/blue/black nodes)
2. **Explanatory context and metadata display** (right blue panel)
3. **Repair/optimization interface** (bottom grey/green block)

---

### ğŸ¯ **Core Interactions & Data Flow**

#### **1. Start / Refresh**

* The system begins with an *optimal MUS* (based on size, weights, or explainability).
* A **step-wise explanation** may accompany it (sequence of constraint propagations).
* The *Refresh* action generates a new MUS distinct from the previous one, possibly based on user preference or diversity heuristics.

#### **2. Select MUS Constraint**

* The user clicks a **constraint node** (red circle) in the MUS visualization.
* On selection:

  * The right-side panel displays metadata:

    * Constraint type, description, involved variables/entities
    * Whether the constraint is soft/hard
  * Related constraints (blue circles) appear â€” those with shared entities, similar semantics, or same type.
  * The system opens the **MCS view** showing correction subsets (green circles) that contain this constraint.

#### **3. Select MCS / Related Constraint**

* User can navigate from a MUS constraint to related constraints or directly to an MCS candidate.
* Each MCS shows which constraints it removes (green circles), possibly overlapping with the current MUS.

#### **4. Select Full MCS (Repair Proposal)**

* User chooses an MCS to fix the conflict â€” this is effectively a â€œrepair plan.â€
* The system can show simulated results (before/after fix).
* Potential extension: switch to a Pareto Simulated Annealing view to explore alternative relaxations or trade-offs (useful when slack variables are available).

---

### âš™ï¸ **Optimization Criteria**

Optimal MUS or MCS selection can be based on:

* Minimal size or minimal weighted size
* Least slack required
* High explainability (smallest step count, most interpretable propagation)
* Coherence of related constraints
* User feedback (e.g., favoring constraints of certain types)

Feedback influences the ranking algorithm (WP3 connection â€” â€œlearning from feedbackâ€).

---

### ğŸ§® **Repair Enumeration & Ranking**

Enumerate all MCSes containing the selected constraint:

* Rank by:

  * Size/weights (smallest = most preferred)
  * Overlap with other MUS constraints
  * Inclusion of related constraints
  * Learned user preference model
* The ranking determines visual order or opacity in the bottom grey panel.

---

### âœ… **Advantages / Disadvantages (from notes)**

**Advantages:**

* Intuitive, guided exploration with freedom of choice.
* Naturally integrates stepwise explanation and repair workflow.
* Prioritizes understandability (ranked, interpretable explanations).

**Disadvantages:**

* Doesnâ€™t allow one-by-one manual constraint removal.
* Difficult to tightly integrate with continuous â€œslackâ€ relaxation models.
* May be less scalable for hundreds of constraints without clustering or aggregation.

---

### ğŸ§­ **Interpretation of Visual Encoding**

| Color          | Meaning                         | Context                                    |
| -------------- | ------------------------------- | ------------------------------------------ |
| ğŸ”´ Red         | Constraint in current MUS       | conflict set (unsatisfiable)               |
| ğŸŸ¢ Green       | Constraint in MCS               | removal restores feasibility               |
| ğŸ”µ Blue        | Related constraint              | connected via shared entities or semantics |
| âš« Black        | Selected constraint             | currently active focus in explanation      |
| ğŸŸ¦ Blue Panel  | Constraint metadata + relations | right info panel                           |
| ğŸŸ© Green Panel | MCS enumeration & ranking       | repair options list                        |
| ğŸŸ¥ Red Box     | MUS workspace                   | current explanation view                   |
| ğŸ©¶ Grey Box    | Repair workspace                | candidate solutions zone                   |

Dashed arrows represent *interactive transitions*:

* from explanation â†’ repair
* from constraint â†’ related subset

---

### ğŸ’¡ **Conceptual Summary**

This design implements the *Explainâ€“Repair Loop* visually:

1. Explain infeasibility â†’ visualize MUS (conflict)
2. Explore cause â†’ view constraint metadata + relations
3. Explore fix â†’ enumerate & rank MCSes
4. Apply or simulate repair â†’ move back to feasible region


---

## ğŸ§­ Why This Is the Right Architectural Center

### 1. It keeps the humanâ€™s *mental model* intact.

Schedulers, coordinators, and instructors all think in *spatio-temporal terms*:
â€œWhich exam is when, in which room, with which invigilator?â€
If explanations, infeasibilities, or repairs are all framed **relative to that familiar grid**, the dashboard speaks the userâ€™s language instead of the solverâ€™s.

### 2. It makes explanation *contextual* instead of abstract.

Rather than showing a detached graph of constraints (which is cognitively distant), you visualise violations **in situ**:

* A conflicting exam cell glows red.
* Hovering reveals â€œroom capacity exceeded / student overlap / invigilator clash.â€
* Clicking shows the MUS containing those constraints.
  Users can see *where* the issue lives before they dive into *why*.

### 3. It naturally supports **progressive disclosure**.

* **Level 1:** Default view = valid or current best schedule.
* **Level 2:** Toggle on â€œExplain conflictsâ€ â†’ overlay constraint indicators.
* **Level 3:** Click one â†’ open explanation panel (MUS/MCS info).
* **Level 4:** â€œShow repair optionsâ€ â†’ side panel offers feasible swaps or relaxations.
  Users stay anchored; only the surrounding context changes.

### 4. It fits the *Explain â†’ Explore â†’ Repair* loop visually.

| Stage       | User action                      | System response                                   |
| ----------- | -------------------------------- | ------------------------------------------------- |
| **Explain** | Select an exam cell              | Highlight conflicting constraints (MUS overlay)   |
| **Explore** | Toggle alternative slots/rooms   | Show counterfactual schedules + constraint deltas |
| **Repair**  | Accept change / relax constraint | Update model & re-solve incrementally             |

---

## ğŸ§© How to Structure the Dashboard Around the Schedule

### **Primary Canvas: Exam Schedule Grid**

* Calendar or matrix: days Ã— time slots Ã— rooms.
* Cells encode assigned exams; color = faculty, course size, or status.
* Overlay layers:

  * ğŸ”´ constraint violations (conflicts)
  * ğŸŸ¢ feasible swap candidates
  * ğŸŸ£ slack magnitude / preference weight

### **Secondary Panels (contextual)**

| Panel             | Purpose                                            | Appears when                    |
| ----------------- | -------------------------------------------------- | ------------------------------- |
| **Right sidebar** | *Explanation/Repair panel*                         | User selects a conflict or exam |
| **Bottom panel**  | *Solver feedback timeline / Step-wise propagation* | User requests â€œwhy infeasible?â€ |
| **Left panel**    | *Constraint filters / scenario controls*           | Always visible (global view)    |

### **Example Interaction Flow**

1. Scheduler loads timetable â†’ sees color-coded feasible schedule.
2. â€œHighlight conflictsâ€ button adds transparent overlays for violated soft constraints.
3. Hover on red slot â†’ tooltip summarises reason.
4. Click â†’ sidebar expands:

   * â€œConflict set (MUS)â€
   * â€œRepair options (MCS)â€
   * â€œImpact previewâ€ button showing what changes if relaxed.
5. Accept repair â†’ animation updates grid; conflicts fade.

This flow mirrors how professionals already work with scheduling spreadsheets but enriches them with solver intelligence.

---

## ğŸ§® Data-to-Visualization Mapping for Exam Scheduling

| Data                            | Visual encoding                 | Interaction                           |
| ------------------------------- | ------------------------------- | ------------------------------------- |
| Exam allocation                 | Cell position (time Ã— room)     | drag-drop or click                    |
| Hard constraint violation       | Red outline                     | tooltip â€œhard constraint unsatisfiedâ€ |
| Soft constraint violation       | Amber fill, opacity âˆ weight    | hover for penalty                     |
| Related exams (shared students) | Ghost highlights in other cells | hover a course                        |
| Alternative feasible slot       | Green halo                      | click to simulate repair              |
| MUS/MCS membership              | Border pattern                  | toggle layer                          |

---

## âš™ï¸ Implementation Implications

* **Backend:** CPMpy incremental solving â†’ fast re-solves for drag-and-drop repairs.
* **Front-end:**

  * Central D3/Canvas scheduler component.
  * Linked state store for selected constraint / explanation context.
  * Side panels request data via `/api/explain/mus?exam_id=...`, `/api/repair/options`.
* **User flow continuity:** the solver never feels like a separate entity; itâ€™s just a â€œsmart assistantâ€ inside the schedule.

---

## ğŸ§  Design Recommendations

1. **Keep the schedule visually calm** â€” conflicts and explanations appear only on demand.
2. **Use animated transitions** to preserve spatial continuity when switching between feasible / repaired views.
3. **Use natural metaphors** for explanation categories:

   * â€œOverlapsâ€ = lines between cells
   * â€œCapacity limitsâ€ = bars filling room cell
   * â€œFairness issuesâ€ = color gradients across days
4. **Provide reversible actions** â€” users can always undo a repair or compare alternatives side-by-side.
5. **Add a summary strip** showing global metrics (coverage, fairness, objective score) that updates live after each repair.

---

## âœ… Bottom Line

Centering the dashboard on the **feasible schedule** is both cognitively aligned and interaction-efficient.
Explanations and repairs become **spatially grounded events** rather than abstract algorithmic states.
It allows:

* Direct manipulation (â€œshow me why this exam is problematicâ€)
* Immediate visual feedback (â€œhereâ€™s how to fix itâ€)
* Seamless learning loop for users and model alike.

---

Would you like me to sketch three **layout variations** for this schedule-centric dashboard (minimal analytic, explanatory-rich, and what-if exploration) so you can compare visual composition and user flow before committing to one direction?




